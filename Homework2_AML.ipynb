{"cells":[{"cell_type":"markdown","metadata":{"id":"qf3z5SqWZ91b"},"source":["# Torch"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36612,"status":"ok","timestamp":1700589869346,"user":{"displayName":"Davide Vitabile","userId":"10252235014316076173"},"user_tz":-60},"id":"YbtEmI1AiTkF","outputId":"56526a77-519d-4840-f130-94170c0d6ead"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["import torch\n","\n","\n","# use GPU if available\n","\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  #'cpu' # 'cuda' or 'cpu'\n","\n","print(DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"czjvnq3FjBmh"},"source":["# Download Dataset GTEA61\n","For Google Colab"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":147076,"status":"ok","timestamp":1700590037711,"user":{"displayName":"Davide Vitabile","userId":"10252235014316076173"},"user_tz":-60},"id":"GBkwNEyc_09F","outputId":"453cd5e3-2eb1-4a84-9fe6-30694be199ab"},"outputs":[{"data":{"text/plain":["' \\n\\nfrom google.colab import drive\\nimport os\\ndrive.mount(\\'/content/drive\\')\\nimport sys, os\\n\\nif not os.path.isfile(\\'/content/GTEA61\\'):\\n  !unzip  \"/content/drive/MyDrive/Colab Notebooks/GTEA61.zip\" -d \"/content/\"\\n\\nif not os.path.isdir(\\'/content/GTEA61\\'):\\n  print(\"Dataset doesn\\'t exist\")\\n\\n#Weights\\nif not os.path.isfile(\"/content/best_model_state_dict_rgb_split2.pth\"):\\n  !gdown --id 1B7Xh6hQ9Py8fmL-pjmLzlCent6dnuex5 # 3-5 min\\n\\n'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["''' \n","\n","from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","\n","import sys, os\n","\n","\n","if not os.path.isfile('/content/GTEA61'):\n","\n","  !unzip  \"/content/drive/MyDrive/Colab Notebooks/GTEA61.zip\" -d \"/content/\"\n","\n","\n","if not os.path.isdir('/content/GTEA61'):\n","\n","  print(\"Dataset doesn't exist\")\n","\n","\n","#Weights\n","\n","if not os.path.isfile(\"/content/best_model_state_dict_rgb_split2.pth\"):\n","\n","  !gdown --id 1B7Xh6hQ9Py8fmL-pjmLzlCent6dnuex5 # 3-5 min\n","\n","  \n","# original\n","\n","import os\n","\n","#1YKfdhB9Xxh4pmND1V3gcm3Gyjc8v8idq\n","if not os.path.isfile('/content/GTEA61.zip'):\n","  !gdown --id 1Z5RWA8yKIy0PvxMlScV-aAz22ITtivfk # 3-5 min\n","  !jar xvf  \"/content/GTEA61.zip\"\n","\n","if not os.path.isdir('/content/GTEA61'):\n","  print(\"Dataset doesn't exist\")\n","\n","#Weights\n","if not os.path.isfile(\"/content/best_model_state_dict_rgb_split2.pth\"):\n","  !gdown --id 1B7Xh6hQ9Py8fmL-pjmLzlCent6dnuex5 # 3-5 min\n","\n","'''"]},{"cell_type":"markdown","metadata":{"id":"Kk0rtAlnSlDF"},"source":["\n","# Download Code"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1931,"status":"ok","timestamp":1700591839274,"user":{"displayName":"Davide Vitabile","userId":"10252235014316076173"},"user_tz":-60},"id":"Z8xcWfReaSd_","outputId":"c025be60-cef4-479f-b4f9-277400598441"},"outputs":[{"name":"stderr","output_type":"stream","text":["Cloning into 'Homework_AIML'...\n"]}],"source":["!git clone \"https://github.com/plana93/Homework_AIML.git\""]},{"cell_type":"markdown","metadata":{"id":"jiwWzjzSio-h"},"source":["\n","\n","# Import Code\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2979,"status":"ok","timestamp":1700591844743,"user":{"displayName":"Davide Vitabile","userId":"10252235014316076173"},"user_tz":-60},"id":"wl6fSd3MXofW"},"outputs":[],"source":["import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.backends import cudnn\n","from colorama import init\n","from colorama import Fore, Back, Style\n","\n","from torchvision.models import resnet34\n","from PIL import Image\n","from tqdm import tqdm\n","\n","import sys\n","\n","sys.path.append(\"Homework_AIML/\")\n","import Homework_AIML\n","from Homework_AIML import *\n","\n","from gtea_dataset import GTEA61, GTEA61_flow, GTEA61_2Stream\n","from spatial_transforms import (\n","    Compose,\n","    ToTensor,\n","    CenterCrop,\n","    Scale,\n","    Normalize,\n","    MultiScaleCornerCrop,\n","    RandomHorizontalFlip,\n",")"]},{"cell_type":"markdown","metadata":{"id":"g1GznJhObXPk"},"source":["# **Learning without Temporal information** (avgpool)"]},{"cell_type":"markdown","metadata":{"id":"Sy4KrHClbAmC"},"source":["## MAIN PARAMs"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":449,"status":"ok","timestamp":1700591851423,"user":{"displayName":"Davide Vitabile","userId":"10252235014316076173"},"user_tz":-60},"id":"w-tz9mHPbCYW"},"outputs":[],"source":["homework_step = 0  # --> Learning without Temporal information (avgpool)\n","# homework_step = 1 #--> Learning with Temporal information (LSTM)\n","# homework_step = 2 #--> Learning with Spatio-Temporal information (ConvLSTM)\n","\n","\n","DATA_DIR = 'datasets/GTEA61/'  # path dataset\n","model_folder = 'saved_models/' + \"homework_step\" + str(homework_step) + \"/\"  # path to save model\n","if not os.path.isdir(model_folder):\n","    os.makedirs(model_folder)\n","\n","\n","# All this param can be change!\n","\n","NUM_CLASSES = 61\n","BATCH_SIZE = 64\n","LR = 0.001  # The initial Learning Rate\n","MOMENTUM = 0.9  # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n","NUM_EPOCHS = 200  # Total number of training epochs (iterations over dataset)\n","STEP_SIZE = [25, 75, 150]  # How many epochs before decreasing learning rate (if using a step-down policy)\n","GAMMA = 0.1  # Multiplicative factor for learning rate step-down\n","MEM_SIZE = 512  # Dim of internal state of LSTM or ConvLSTM\n","SEQ_LEN = 3  # Num Frames\n","\n","# this dictionary is needed for the logger class\n","parameters = {\n","    'DEVICE': DEVICE,\n","    'NUM_CLASSES': NUM_CLASSES,\n","    'BATCH_SIZE': BATCH_SIZE,\n","    'LR': LR,\n","    'MOMENTUM': MOMENTUM,\n","    'WEIGHT_DECAY': WEIGHT_DECAY,\n","    'NUM_EPOCHS': NUM_EPOCHS,\n","    'STEP_SIZE': STEP_SIZE,\n","    'GAMMA': GAMMA,\n","    'MEM_SIZE': MEM_SIZE,\n","    'SEQ_LEN': SEQ_LEN,\n","}"]},{"cell_type":"markdown","metadata":{"id":"UPwkOR8taVdN"},"source":["## Dataloaders & Preprocessing"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1700591853618,"user":{"displayName":"Davide Vitabile","userId":"10252235014316076173"},"user_tz":-60},"id":"LT_Gy79SgBLq"},"outputs":[],"source":["# Normalize\n","normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","spatial_transform = Compose(\n","    [Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224), ToTensor(), normalize]\n",")\n","spatial_transform_val = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1700591853618,"user":{"displayName":"Davide Vitabile","userId":"10252235014316076173"},"user_tz":-60},"id":"T69vfGGhjKa_","outputId":"f0dba6f8-c981-4d0c-fdeb-a9bcad122e92"},"outputs":[{"name":"stdout","output_type":"stream","text":["['S1', 'S2', 'S3', 'S4']\n","['S1', 'S2', 'S3', 'S4']\n","Train Dataset: 341\n","Test Dataset: 116\n"]}],"source":["# Prepare Pytorch train/test Datasets\n","train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform, seq_len=SEQ_LEN)\n","test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=SEQ_LEN)\n","\n","# Check dataset sizes\n","print('Train Dataset: {}'.format(len(train_dataset)))\n","print('Test Dataset: {}'.format(len(test_dataset)))\n","\n","# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"]},{"cell_type":"markdown","metadata":{"id":"21wjiwvW-OPQ"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":426,"status":"ok","timestamp":1700591857243,"user":{"displayName":"Davide Vitabile","userId":"10252235014316076173"},"user_tz":-60},"id":"XMWuE-4SHxoY"},"outputs":[],"source":["import torch\n","import resnetMod\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.autograd import Variable\n","\n","\n","# LSTM\n","class MyLSTMCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(MyLSTMCell, self).__init__()\n","\n","    def forward(self, x, state):\n","        if state is None:\n","            state = (\n","                Variable(torch.randn(x.size(0), x.size(1)).cuda()),\n","                Variable(torch.randn(x.size(0), x.size(1)).cuda()),\n","            )\n","\n","        ##################################\n","        # You should implement this part #\n","        ##################################\n","\n","        return None, None\n","\n","\n","# ConvLSTM\n","class MyConvLSTMCell(nn.Module):\n","    def __init__(self, input_size, hidden_size, kernel_size=3, stride=1, padding=1):\n","        super(MyConvLSTMCell, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","        self.conv_i_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n","        self.conv_i_hh = nn.Conv2d(\n","            hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding, bias=False\n","        )\n","\n","        self.conv_f_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n","        self.conv_f_hh = nn.Conv2d(\n","            hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding, bias=False\n","        )\n","\n","        self.conv_c_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n","        self.conv_c_hh = nn.Conv2d(\n","            hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding, bias=False\n","        )\n","\n","        self.conv_o_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n","        self.conv_o_hh = nn.Conv2d(\n","            hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding, bias=False\n","        )\n","\n","        torch.nn.init.xavier_normal_(self.conv_i_xx.weight)\n","        torch.nn.init.constant_(self.conv_i_xx.bias, 0)\n","        torch.nn.init.xavier_normal_(self.conv_i_hh.weight)\n","\n","        torch.nn.init.xavier_normal_(self.conv_f_xx.weight)\n","        torch.nn.init.constant_(self.conv_f_xx.bias, 0)\n","        torch.nn.init.xavier_normal_(self.conv_f_hh.weight)\n","\n","        torch.nn.init.xavier_normal_(self.conv_c_xx.weight)\n","        torch.nn.init.constant_(self.conv_c_xx.bias, 0)\n","        torch.nn.init.xavier_normal_(self.conv_c_hh.weight)\n","\n","        torch.nn.init.xavier_normal_(self.conv_o_xx.weight)\n","        torch.nn.init.constant_(self.conv_o_xx.bias, 0)\n","        torch.nn.init.xavier_normal_(self.conv_o_hh.weight)\n","\n","    def forward(self, x, state):\n","        if state is None:\n","            state = (\n","                Variable(torch.randn(x.size(0), x.size(1), x.size(2), x.size(3)).cuda()),\n","                Variable(torch.randn(x.size(0), x.size(1), x.size(2), x.size(3)).cuda()),\n","            )\n","\n","        ##################################\n","        # You should implement this part #\n","        ##################################\n","\n","        return None, None\n","\n","\n","# Network\n","class ourModel(nn.Module):\n","    def __init__(self, num_classes=61, mem_size=512, homework_step=0, DEVICE=\"\"):\n","        super(ourModel, self).__init__()\n","        self.DEVICE = DEVICE\n","        self.num_classes = num_classes\n","        self.resNet = resnetMod.resnet34(True, True)\n","        self.mem_size = mem_size\n","        self.weight_softmax = self.resNet.fc.weight\n","        self.homework_step = homework_step\n","        if self.homework_step == 1:\n","            self.lstm_cell = MyLSTMCell(512, mem_size)\n","        elif self.homework_step == 2:\n","            self.lstm_cell = MyConvLSTMCell(512, mem_size)\n","\n","        self.avgpool = nn.AvgPool2d(7)\n","        self.dropout = nn.Dropout(0.7)\n","        self.fc = nn.Linear(mem_size, self.num_classes)\n","        self.classifier = nn.Sequential(self.dropout, self.fc)\n","\n","    def forward(self, inputVariable):\n","        # Learning without Temporal information (mean)\n","        if self.homework_step == 0:\n","            video_level_features = torch.zeros((inputVariable.size(1), self.mem_size)).to(self.DEVICE)\n","            for t in range(inputVariable.size(0)):\n","                # spatial_frame_feat: (bs, 512, 7, 7)\n","                _, spatial_frame_feat, _ = self.resNet(inputVariable[t])\n","                # frames_feat: (bs, 512)\n","                frame_feat = self.avgpool(spatial_frame_feat).view(spatial_frame_feat.size(0), -1)\n","                video_level_features = video_level_features + frame_feat\n","\n","            video_level_features = video_level_features / inputVariable.size(0)\n","            logits = self.classifier(video_level_features)\n","            return logits, video_level_features\n","\n","        # Learning with Temporal information (LSTM)\n","        elif self.homework_step == 1:\n","            state = (\n","                torch.zeros((inputVariable.size(1), self.mem_size)).to(self.DEVICE),\n","                torch.zeros((inputVariable.size(1), self.mem_size)).to(self.DEVICE),\n","            )\n","            for t in range(inputVariable.size(0)):\n","                # spatial_frame_feat: (bs, 512, 7, 7)\n","                _, spatial_frame_feat, _ = self.resNet(inputVariable[t])\n","                # frames_feat: (bs, 512)\n","                frame_feat = self.avgpool(spatial_frame_feat).view(state[1].size(0), -1)\n","                state = self.lstm_cell(frame_feat, state)\n","\n","            video_level_features = state[1]\n","            logits = self.classifier(video_level_features)\n","            return logits, video_level_features\n","\n","        # Learning with Temporal information (ConvLSTM)\n","        elif self.homework_step == 2:\n","            state = (\n","                torch.zeros((inputVariable.size(1), self.mem_size, 7, 7)).to(self.DEVICE),\n","                torch.zeros((inputVariable.size(1), self.mem_size, 7, 7)).to(self.DEVICE),\n","            )\n","            for t in range(inputVariable.size(0)):\n","                # spatial_frame_feat: (bs, 512, 7, 7)\n","                _, spatial_frame_feat, _ = self.resNet(inputVariable[t])\n","                state = self.lstm_cell(spatial_frame_feat, state)\n","            video_level_features = self.avgpool(state[1]).view(state[1].size(0), -1)\n","            logits = self.classifier(video_level_features)\n","            return logits, video_level_features"]},{"cell_type":"markdown","metadata":{"id":"Ru8vllrMbgvL"},"source":["## Build Model - Loss - Opt"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7434,"status":"ok","timestamp":1700591872901,"user":{"displayName":"Davide Vitabile","userId":"10252235014316076173"},"user_tz":-60},"id":"XZe-ZbEL7z3x","outputId":"20a3f7cd-72f0-45fc-895a-568058290aa5"},"outputs":[],"source":["# CUDA_LAUNCH_BLOCKING=1\n","validate = True\n","\n","model = ourModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE, homework_step=homework_step, DEVICE=DEVICE)  # model\n","\n","# Train only the lstm cell and classifier\n","model.train(False)\n","for params in model.parameters():\n","    params.requires_grad = False\n","\n","if homework_step > 0:\n","    for params in model.lstm_cell.parameters():\n","        params.requires_grad = True\n","    model.lstm_cell.train(True)\n","\n","for params in model.classifier.parameters():\n","    params.requires_grad = True\n","model.classifier.train(True)\n","\n","\n","model = model.to(DEVICE)\n","\n","# model.load_state_dict(torch.load(\"/content/best_model_state_dict_rgb_split2.pth\", map_location=torch.device('cpu')), strict=True)\n","\n","\n","# Loss\n","loss_fn = nn.CrossEntropyLoss()\n","# Opt\n","trainable_params = [p for p in model.parameters() if p.requires_grad]\n","optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n","# Scheduler\n","optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"]},{"cell_type":"markdown","metadata":{"id":"z0MWgLingzhw"},"source":["## Training\n"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1708222,"status":"ok","timestamp":1700593581119,"user":{"displayName":"Davide Vitabile","userId":"10252235014316076173"},"user_tz":-60},"id":"p-uE2A9eHmtn","outputId":"10b7f226-7696-4e3d-9273-cd66d042ecbd"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[30mTrain: Epoch = 1 | Loss = 4.745 | Accuracy = 2.812\n","\u001b[32mVal: Epoch = 1 | Loss 4.150 | Accuracy = 4.310\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 2 | Loss = 4.509 | Accuracy = 3.125\n","\u001b[32mVal: Epoch = 2 | Loss 4.104 | Accuracy = 6.034\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 3 | Loss = 4.481 | Accuracy = 3.750\n","\u001b[32mVal: Epoch = 3 | Loss 4.035 | Accuracy = 6.034\n","\u001b[30mTrain: Epoch = 4 | Loss = 4.539 | Accuracy = 4.062\n","\u001b[32mVal: Epoch = 4 | Loss 3.945 | Accuracy = 2.586\n","\u001b[30mTrain: Epoch = 5 | Loss = 4.361 | Accuracy = 4.062\n","\u001b[32mVal: Epoch = 5 | Loss 3.877 | Accuracy = 3.448\n","\u001b[30mTrain: Epoch = 6 | Loss = 4.360 | Accuracy = 3.438\n","\u001b[32mVal: Epoch = 6 | Loss 3.827 | Accuracy = 5.172\n","\u001b[30mTrain: Epoch = 7 | Loss = 4.241 | Accuracy = 3.750\n","\u001b[32mVal: Epoch = 7 | Loss 3.784 | Accuracy = 7.759\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 8 | Loss = 4.188 | Accuracy = 5.000\n","\u001b[32mVal: Epoch = 8 | Loss 3.749 | Accuracy = 8.621\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 9 | Loss = 4.150 | Accuracy = 4.688\n","\u001b[32mVal: Epoch = 9 | Loss 3.707 | Accuracy = 10.345\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 10 | Loss = 4.131 | Accuracy = 5.312\n","\u001b[32mVal: Epoch = 10 | Loss 3.673 | Accuracy = 10.345\n","\u001b[30mTrain: Epoch = 11 | Loss = 4.047 | Accuracy = 8.438\n","\u001b[32mVal: Epoch = 11 | Loss 3.636 | Accuracy = 12.069\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 12 | Loss = 3.996 | Accuracy = 6.562\n","\u001b[32mVal: Epoch = 12 | Loss 3.605 | Accuracy = 12.931\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 13 | Loss = 4.042 | Accuracy = 7.187\n","\u001b[32mVal: Epoch = 13 | Loss 3.583 | Accuracy = 9.483\n","\u001b[30mTrain: Epoch = 14 | Loss = 3.952 | Accuracy = 10.625\n","\u001b[32mVal: Epoch = 14 | Loss 3.554 | Accuracy = 8.621\n","\u001b[30mTrain: Epoch = 15 | Loss = 3.829 | Accuracy = 10.312\n","\u001b[32mVal: Epoch = 15 | Loss 3.525 | Accuracy = 11.207\n","\u001b[30mTrain: Epoch = 16 | Loss = 3.948 | Accuracy = 7.812\n","\u001b[32mVal: Epoch = 16 | Loss 3.494 | Accuracy = 12.069\n","\u001b[30mTrain: Epoch = 17 | Loss = 3.971 | Accuracy = 7.500\n","\u001b[32mVal: Epoch = 17 | Loss 3.463 | Accuracy = 13.793\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 18 | Loss = 3.952 | Accuracy = 6.562\n","\u001b[32mVal: Epoch = 18 | Loss 3.432 | Accuracy = 14.655\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 19 | Loss = 3.654 | Accuracy = 13.125\n","\u001b[32mVal: Epoch = 19 | Loss 3.409 | Accuracy = 16.379\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 20 | Loss = 3.897 | Accuracy = 8.125\n","\u001b[32mVal: Epoch = 20 | Loss 3.395 | Accuracy = 18.966\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 21 | Loss = 3.833 | Accuracy = 7.812\n","\u001b[32mVal: Epoch = 21 | Loss 3.378 | Accuracy = 15.517\n","\u001b[30mTrain: Epoch = 22 | Loss = 3.676 | Accuracy = 10.938\n","\u001b[32mVal: Epoch = 22 | Loss 3.357 | Accuracy = 14.655\n","\u001b[30mTrain: Epoch = 23 | Loss = 3.750 | Accuracy = 10.000\n","\u001b[32mVal: Epoch = 23 | Loss 3.343 | Accuracy = 16.379\n","\u001b[30mTrain: Epoch = 24 | Loss = 3.784 | Accuracy = 9.688\n","\u001b[32mVal: Epoch = 24 | Loss 3.313 | Accuracy = 18.103\n","\u001b[30mTrain: Epoch = 25 | Loss = 3.641 | Accuracy = 11.250\n","\u001b[32mVal: Epoch = 25 | Loss 3.278 | Accuracy = 21.552\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 26 | Loss = 3.702 | Accuracy = 10.312\n","\u001b[32mVal: Epoch = 26 | Loss 3.274 | Accuracy = 22.414\n","[||| NEW BEST on val||||]\n","\u001b[30mTrain: Epoch = 27 | Loss = 3.597 | Accuracy = 12.500\n","\u001b[32mVal: Epoch = 27 | Loss 3.270 | Accuracy = 21.552\n","\u001b[30mTrain: Epoch = 28 | Loss = 3.691 | Accuracy = 12.812\n","\u001b[32mVal: Epoch = 28 | Loss 3.267 | Accuracy = 20.690\n","\u001b[30mTrain: Epoch = 29 | Loss = 3.548 | Accuracy = 14.375\n","\u001b[32mVal: Epoch = 29 | Loss 3.263 | Accuracy = 21.552\n","\u001b[30mTrain: Epoch = 30 | Loss = 3.635 | Accuracy = 13.125\n","\u001b[32mVal: Epoch = 30 | Loss 3.260 | Accuracy = 20.690\n","\u001b[30mTrain: Epoch = 31 | Loss = 3.695 | Accuracy = 9.375\n","\u001b[32mVal: Epoch = 31 | Loss 3.257 | Accuracy = 21.552\n","\u001b[30mTrain: Epoch = 32 | Loss = 3.748 | Accuracy = 9.688\n","\u001b[32mVal: Epoch = 32 | Loss 3.254 | Accuracy = 20.690\n","\u001b[30mTrain: Epoch = 33 | Loss = 3.627 | Accuracy = 13.125\n","\u001b[32mVal: Epoch = 33 | Loss 3.252 | Accuracy = 20.690\n","\u001b[30mTrain: Epoch = 34 | Loss = 3.608 | Accuracy = 12.500\n","\u001b[32mVal: Epoch = 34 | Loss 3.249 | Accuracy = 19.828\n","\u001b[30mTrain: Epoch = 35 | Loss = 3.595 | Accuracy = 11.250\n","\u001b[32mVal: Epoch = 35 | Loss 3.247 | Accuracy = 19.828\n","\u001b[30mTrain: Epoch = 36 | Loss = 3.573 | Accuracy = 12.812\n","\u001b[32mVal: Epoch = 36 | Loss 3.244 | Accuracy = 20.690\n","\u001b[31mEARLY STOPPING\n","\u001b[36mBest Acc -->  22.413793103448278\n","\u001b[36mLast Acc -->  20.689655172413794\n"]}],"source":["PATIENCE = 10\n","\n","patience_count = 0\n","\n","train_iter = 0\n","\n","val_iter = 0\n","\n","min_accuracy = 0\n","\n","\n","trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n","val_samples = len(test_dataset)\n","\n","iterPerEpoch = len(train_loader)\n","val_steps = len(val_loader)\n","\n","cudnn.benchmark\n","\n","model_checkpoint = \"model\"  # name\n","\n","\n","\n","for epoch in range(NUM_EPOCHS):\n","\n","    epoch_loss = 0\n","\n","    numCorrTrain = 0\n","\n","\n","    # blocks to train\n","\n","    if homework_step > 0:\n","\n","        model.lstm_cell.train(True)\n","\n","    model.classifier.train(True)\n","\n","\n","    for i, (inputs, targets) in enumerate(train_loader):\n","\n","        train_iter += 1\n","\n","        optimizer_fn.zero_grad()\n","\n","\n","        # (BS, Frames, C, W, H) --> (Frames, BS, C, W, H)\n","\n","        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n","\n","        labelVariable = targets.to(DEVICE)\n","\n","\n","        # feeds in model\n","\n","        output_label, _ = model(inputVariable)\n","\n","\n","        # compute loss\n","\n","        loss = loss_fn(output_label, labelVariable)\n","\n","\n","        # backward loss and optimizer step\n","\n","        loss.backward()\n","\n","        optimizer_fn.step()\n","\n","\n","        # compute the training accuracy\n","\n","        _, predicted = torch.max(output_label.data, 1)\n","\n","        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n","        step_loss = loss.data.item()\n","\n","        epoch_loss += step_loss\n","\n","\n","    avg_loss = epoch_loss / iterPerEpoch\n","\n","    trainAccuracy = (numCorrTrain / trainSamples) * 100\n","\n","    # train_logger.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n","    print(\n","        Fore.BLACK + 'Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_loss, trainAccuracy)\n","    )\n","\n","    if validate:\n","\n","        if (epoch + 1) % 1 == 0:\n","\n","            model.train(False)\n","\n","            val_loss_epoch = 0\n","\n","            numCorr = 0\n","\n","            for j, (inputs, targets) in enumerate(val_loader):\n","\n","                val_iter += 1\n","\n","                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n","\n","                labelVariable = targets.to(DEVICE)\n","\n","\n","                output_label, _ = model(inputVariable)\n","\n","                val_loss = loss_fn(output_label, labelVariable)\n","                val_loss_step = val_loss.data.item()\n","\n","                val_loss_epoch += val_loss_step\n","\n","                _, predicted = torch.max(output_label.data, 1)\n","\n","                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n","\n","                # val_logger.add_step_data(val_iter, numCorr, val_loss_step)\n","\n","\n","            val_accuracy = (numCorr / val_samples) * 100\n","\n","            avg_val_loss = val_loss_epoch / val_steps\n","\n","            print(\n","                Fore.GREEN\n","                + 'Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy)\n","            )\n","\n","            if val_accuracy > min_accuracy:\n","\n","                print(\"[||| NEW BEST on val||||]\")\n","                patience_count = 0\n","\n","                save_path_model = os.path.join(model_folder, model_checkpoint)\n","\n","                torch.save(model.state_dict(), save_path_model)\n","\n","                min_accuracy = val_accuracy\n","            elif val_accuracy < min_accuracy:\n","                patience_count += 1\n","\n","        if patience_count == PATIENCE:\n","            print(Fore.RED + 'EARLY STOPPING')\n","            break\n","\n","    optim_scheduler.step()\n","\n","\n","print(Fore.CYAN + \"Best Acc --> \", min_accuracy)\n","\n","print(Fore.CYAN + \"Last Acc --> \", val_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"lrLs_T2Qd0kc"},"source":["## Test"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2677,"status":"ok","timestamp":1700593844058,"user":{"displayName":"Davide Vitabile","userId":"10252235014316076173"},"user_tz":-60},"id":"gqK1ExB0cl8D","outputId":"02c99f4c-9815-45a0-fcf5-0c0e921a4a9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss 3.244 | Accuracy = 20.690\n"]}],"source":["model.train(False)\n","val_loss_epoch = 0\n","numCorr = 0\n","val_iter = 0\n","val_samples = len(test_dataset)\n","val_steps = len(val_loader)\n","\n","with torch.no_grad():\n","    for j, (inputs, targets) in enumerate(val_loader):\n","        val_iter += 1\n","        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n","        labelVariable = targets.to(DEVICE)\n","\n","        output_label, _ = model(inputVariable)\n","        val_loss = loss_fn(output_label, labelVariable)\n","        val_loss_step = val_loss.data.item()\n","        val_loss_epoch += val_loss_step\n","        _, predicted = torch.max(output_label.data, 1)\n","        numCorr += torch.sum(predicted == labelVariable.data).data.item()\n","\n","    val_accuracy = (numCorr / val_samples) * 100\n","    avg_val_loss = val_loss_epoch / val_steps\n","\n","print('Loss {:.3f} | Accuracy = {:.3f}'.format(avg_val_loss, val_accuracy))"]},{"cell_type":"markdown","metadata":{"id":"Gr9BxL8zeBfv"},"source":["# **Learning with Temporal information** (LSTM)"]},{"cell_type":"markdown","metadata":{"id":"s-qHYgnyf_wn"},"source":["# **Learning with Spatio-Temporal information** (ConvLSTM)\n","\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["g1GznJhObXPk","Sy4KrHClbAmC","UPwkOR8taVdN","Ru8vllrMbgvL","Gr9BxL8zeBfv"],"gpuType":"T4","provenance":[{"file_id":"14xCV5WEXQaaaenwdqs_ynaHn8ZmvL61X","timestamp":1700588775265}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
